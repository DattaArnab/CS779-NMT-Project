{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT =  \"This is a cat .\"\n",
    "# Set to an English sentence to translate; if None, reads from stdin or input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# User-configurable variables\n",
    "# -------------------------------\n",
    "CPU = False  # Set True to force CPU inference\n",
    "\n",
    "# Model hyperparameters (must match training)\n",
    "HIDDEN_SIZE = 333\n",
    "NHEADS = 9\n",
    "ENC_LAYERS = 4\n",
    "DEC_LAYERS = 4\n",
    "DROPOUT = 0.15\n",
    "DIM_FF = -1  # If <= 0, will default to 4 * HIDDEN_SIZE\n",
    "\n",
    "# Decoding parameters\n",
    "BEAM_WIDTH = 8\n",
    "MAX_LEN = 150\n",
    "LENGTH_PENALTY = 1\n",
    "NO_REPEAT_NGRAM = 3\n",
    "REPETITION_PENALTY = 2\n",
    "MIN_LENGTH = 5\n",
    "TEMPERATURE = 3\n",
    "# Languages to translate into\n",
    "LANGS = [\"Bengali\", \"Hindi\"]\n",
    "# -------------------------------\n",
    "\n",
    "# Reserved tokens\n",
    "RESERVED_TOKENS = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
    "WHITESPACE_MARKER = \"â–\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download, HfApi  # ensures hub deps available via utils.prepare\n",
    "from worker import TransformerEncoder, TransformerDecoder  # model definitions\n",
    "from utils import (\n",
    "    prepare,\n",
    "    detokenize_sentence,\n",
    "    encode_and_pad,\n",
    "    beam_search_translate_transformer,\n",
    "    _decode_token_text,\n",
    ")\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BASE = \"/kaggle/working/\" if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ else \"\"\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdcd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Unicode NFKC normalization and lowercasing to match training flow\n",
    "    normalized = unicodedata.normalize(\"NFKC\", text)\n",
    "    return normalized.casefold()\n",
    "\n",
    "def tokenize_sentence(text: str, tokenizer_data):\n",
    "    # Greedy longest-match tokenization over prepared vocab\n",
    "    vocab, _ = tokenizer_data\n",
    "    vocab_map = {_decode_token_text(tok): i for i, tok in enumerate(vocab)}\n",
    "    text = normalize_text(text)\n",
    "    processed_text = WHITESPACE_MARKER + re.sub(r\"\\s+\", WHITESPACE_MARKER, text.strip())\n",
    "    ids = []\n",
    "    i = 0\n",
    "    while i < len(processed_text):\n",
    "        longest_match = \"\"\n",
    "        for j in range(len(processed_text), i, -1):\n",
    "            sub = processed_text[i:j]\n",
    "            if sub in vocab_map:\n",
    "                longest_match = sub\n",
    "                break\n",
    "        if longest_match:\n",
    "            ids.append(vocab_map[longest_match])\n",
    "            i += len(longest_match)\n",
    "        else:\n",
    "            ids.append(vocab_map[\"<unk>\"])\n",
    "            i += 1\n",
    "    return ids\n",
    "\n",
    "def load_models_and_tokenizers(lang: str, device):\n",
    "    # Load prepared tokenizers bundle from HF using utils.prepare\n",
    "    # Prepared file contains: en_tokenizer_data, target_tokenizer_data\n",
    "    _, en_tok_data, tgt_tok_data, _, _ = prepare(lang)\n",
    "    en_map = {_decode_token_text(tok): i for i, tok in enumerate(en_tok_data[0])}\n",
    "    tgt_map = {_decode_token_text(tok): i for i, tok in enumerate(tgt_tok_data[0])}\n",
    "    en_vocab_size = len(en_tok_data[0])\n",
    "    tgt_vocab_size = len(tgt_tok_data[0])\n",
    "    SOS_ID = tgt_map[\"<s>\"]\n",
    "    EOS_ID = tgt_map[\"</s>\"]\n",
    "    PAD_ID_SRC = en_map[\"<pad>\"]\n",
    "    PAD_ID_TGT = tgt_map[\"<pad>\"]\n",
    "    # Model hyperparameters should match training\n",
    "    d_model = HIDDEN_SIZE\n",
    "    nheads = NHEADS\n",
    "    num_layers_enc = ENC_LAYERS\n",
    "    num_layers_dec = DEC_LAYERS\n",
    "    dim_ff = DIM_FF if DIM_FF and DIM_FF > 0 else 4 * d_model\n",
    "    dropout = DROPOUT\n",
    "    encoder = TransformerEncoder(\n",
    "        en_vocab_size, d_model, nheads, num_layers_enc, dim_ff, dropout, PAD_ID_SRC\n",
    "    ).to(device)\n",
    "    decoder = TransformerDecoder(\n",
    "        d_model, tgt_vocab_size, nheads, num_layers_dec, dim_ff, dropout, PAD_ID_TGT\n",
    "    ).to(device)\n",
    "    # Resolve weight paths\n",
    "    user_secrets = UserSecretsClient()\n",
    "    enc_path = os.path.join(BASE, f\"encoder_{lang}.pt\")\n",
    "    dec_path = os.path.join(BASE, f\"decoder_{lang}.pt\")\n",
    "    if not os.path.exists(enc_path) or not os.path.exists(dec_path):\n",
    "        enc_path = hf_hub_download(\n",
    "            repo_id=\"Arnab-Datta-240185/CS779-Capstone-Project\",\n",
    "            filename=f\"encoder_{lang}.pt\",\n",
    "            repo_type=\"dataset\",\n",
    "            token=user_secrets.get_secret(\"hf_token\")\n",
    "        )\n",
    "        dec_path = hf_hub_download(\n",
    "            repo_id=\"Arnab-Datta-240185/CS779-Capstone-Project\",\n",
    "            filename=f\"decoder_{lang}.pt\",\n",
    "            repo_type=\"dataset\",\n",
    "            token=user_secrets.get_secret(\"hf_token\")\n",
    "        )\n",
    "    encoder.load_state_dict(torch.load(enc_path, map_location=device))\n",
    "    decoder.load_state_dict(torch.load(dec_path, map_location=device))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    return {\n",
    "        \"encoder\": encoder,\n",
    "        \"decoder\": decoder,\n",
    "        \"en_tok_data\": en_tok_data,\n",
    "        \"tgt_tok_data\": tgt_tok_data,\n",
    "        \"en_map\": en_map,\n",
    "        \"SOS_ID\": SOS_ID,\n",
    "        \"EOS_ID\": EOS_ID,\n",
    "        \"PAD_ID_SRC\": PAD_ID_SRC,\n",
    "    }\n",
    "\n",
    "def translate_text(text: str, lang: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not CPU else \"cpu\")\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    bundle = load_models_and_tokenizers(lang, device)\n",
    "\n",
    "    # Tokenize and encode source\n",
    "    src_ids = tokenize_sentence(text, bundle[\"en_tok_data\"])\n",
    "    src_padded = encode_and_pad(bundle[\"en_map\"], src_ids, MAX_LEN)\n",
    "    src_tensor = torch.tensor(src_padded, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # Decode via beam search\n",
    "    out_ids = beam_search_translate_transformer(\n",
    "        bundle[\"encoder\"],\n",
    "        bundle[\"decoder\"],\n",
    "        src_tensor,\n",
    "        bundle[\"SOS_ID\"],\n",
    "        bundle[\"EOS_ID\"],\n",
    "        src_pad_id=bundle[\"PAD_ID_SRC\"],\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        max_length=MAX_LEN,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        min_length=MIN_LENGTH,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    return detokenize_sentence(out_ids, bundle[\"tgt_tok_data\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        text = TEXT\n",
    "\n",
    "        for lang in LANGS:\n",
    "            translation = translate_text(text, lang)\n",
    "            print(translation)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS779",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
